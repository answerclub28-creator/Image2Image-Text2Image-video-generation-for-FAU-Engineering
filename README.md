# Image2Image + Text2Image Video Generation for FAU Engineering

**Course:** CAP6415 â€“ Computer Vision (Fall 2025)  
**Team:** Akhileshwar Reddy Bommineni, Manaswini Pasupuleti, Pathri Jaydeep  
**Instructor:** Prof. Velibor Adzic (vadzic@fau.edu)
**University:** Florida Atlantic University â€“ College of Engineering and Computer Science  

---

## ğŸ¯ Abstract
AI-generated visualizations play an increasingly large role in engineering research, simulations, and educational demonstrations.
This project covers a **hybrid AI video generation pipeline** based on **Text-to-Image (T2I)** and **Image-to-Image (I2I)** transformation concepts.

The system conceptually follows three major stages:

1. **Text-to-Image Generation:**
A frame is generated from a descriptive engineering-themed prompt; examples include "*FAU engineering drone lab with multiple sensors and LED lighting*".

Currently, this step is simulated using a lightweight **mock diffusion pipeline** to ensure **full reproducibility across all devices** without requiring GPUs or large model downloads.
2. **Image-to-Image Refinement:**

The initial frame can be refined or changed so as to simulate small variations including those resulting from lighting, motion, and angle changes.

A lightweight placeholder version is implemented to demonstrate the pipeline architecture. 
3. **Video Assembly:** These generated or refined frames are combined to form a short engineering-themed video clip using some simple frame-to-video utility. 

The whole repository is meant to be **portable**, **lightweight**, and **fully reproducible** on any system, Windows, macOS, Linux, without specific hardware or authentication requirements.
---

## âš™ï¸ Installation & Setup

### 1ï¸âƒ£ Clone the repository
```bash
git clone https://github.com/akhileshwarreddy1706/Image2Image-Text2Image-video-generation-for-FAU-Engineering.git
cd Image2Image-Text2Image-video-generation-for-FAU-Engineering
```
### 2ï¸âƒ£ (Recommended) Create a virtual environment
```bash
python -m venv .venv
source .venv/bin/activate     # (on macOS/Linux)
# OR
.venv\Scripts\activate        # (on Windows)
```
### 3ï¸âƒ£ Install dependencies
```bash
pip install -r requirements.txt
```
### 4ï¸âƒ£ (Optional) Create the environment via Conda
```bash
conda env create -f environment.yml
conda activate cap6415-img2vid
```
## Repository Structure

```bash
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ generate_frames.py        # Mock Text-to-Image frame generator (Week 3)
â”‚   â”œâ”€â”€ refine_frames.py          # Image-to-Image refinement placeholder
â”‚   â””â”€â”€ frames_to_video.py        # Combine frames into a video (FFMPEG/ImageIO)
â”‚
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ week2_frames/             # Frames generated during Week 2
â”‚   â”œâ”€â”€ week3_frames/             # Mock ONNX frames (Week 3 output)
â”‚   â””â”€â”€ frame_000.png             # Example starting frame
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ onnx/                     # Contains config files only (no large ONNX binaries)
â”‚
â”œâ”€â”€ week1log.txt                  # Weekly progress log
â”œâ”€â”€ week2log.txt
â”œâ”€â”€ week3log.txt
â”‚
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ environment.yml               # Conda environment file
â”‚
â”œâ”€â”€ demo_video_script.md          # Outline for final project presentation video
â”œâ”€â”€ LICENSE                       # MIT License
â””â”€â”€ README.md                     #Projectdocumentation (this file)
```


## Running the Project

### Generate frames (mock Text-to-Image)
```bash 
python src/generate_frames.py
```
### Convert frames to video
```bash
python src/frames_to_video.py
```
